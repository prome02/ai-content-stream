interface OllamaConfig {
  baseUrl?: string          // API åŸºç¤ URL
  defaultModel?: string      // é è¨­æ¨¡å‹
  timeout?: number           // è«‹æ±‚é€¾æ™‚ï¼ˆæ¯«ç§’ï¼‰
  maxRetries?: number        // æœ€å¤§é‡è©¦æ¬¡æ•¸
  retryDelay?: number        // é‡è©¦å»¶é²ï¼ˆæ¯«ç§’ï¼‰
}

interface OllamaRequest {
  model: string
  messages: Array<{
    role: 'system' | 'user' | 'assistant'
    content: string
  }>
  stream?: boolean
  options?: {
    temperature?: number
    top_p?: number
    top_k?: number
    repeat_penalty?: number
    num_predict?: number
  }
}

interface OllamaResponse {
  model: string
  created_at: string
  message: {
    role: string
    content: string
  }
  done: boolean
  total_duration?: number
  load_duration?: number
  prompt_eval_count?: number
  prompt_eval_duration?: number
  eval_count?: number
  eval_duration?: number
}

class OllamaError extends Error {
  constructor(
    message: string,
    public statusCode?: number,
    public isRetriable: boolean = true
  ) {
    super(message)
    this.name = 'OllamaError'
  }
}

export class OllamaClient {
  private readonly config: Required<OllamaConfig>
  private readonly controller: AbortController

  constructor(config: OllamaConfig = {}) {
    this.config = {
      baseUrl: config.baseUrl || 'http://localhost:11434',
      defaultModel: config.defaultModel || 'gemma3:4b',
      timeout: config.timeout || 30000, // 30ç§’
      maxRetries: config.maxRetries || 3,
      retryDelay: config.retryDelay || 1000,
    }
    
    this.controller = new AbortController()
  }

  /**
   * ç™¼é€èŠå¤©è«‹æ±‚
   */
  async chat(
    request: OllamaRequest,
    options: { signal?: AbortSignal } = {}
  ): Promise<OllamaResponse> {
    let lastError: OllamaError | null = null
    
    for (let attempt = 0; attempt <= this.config.maxRetries; attempt++) {
      try {
        const timeoutPromise = this.createTimeoutPromise()
        const fetchPromise = this.sendRequest(request, options)
        
        const response = await Promise.race([fetchPromise, timeoutPromise])
        
        if (response instanceof OllamaError) {
          throw response
        }
        
        return response
        
      } catch (error) {
        lastError = error as OllamaError
        
        // æª¢æŸ¥æ˜¯å¦è¦é‡è©¦
        if (attempt < this.config.maxRetries && this.shouldRetry(error)) {
          console.warn(`Ollama é‡è©¦ (${attempt + 1}/${this.config.maxRetries}):`, error instanceof Error ? error.message : String(error))
          
          // æŒ‡æ•¸é€€é¿
          const delay = this.config.retryDelay * Math.pow(2, attempt)
          await this.delay(delay)
          continue
        }
        
        throw this.normalizeError(error)
      }
    }
    
    throw lastError || new OllamaError('Ollama è«‹æ±‚å¤±æ•—')
  }

  /**
   * ç™¼é€ç”Ÿæˆè«‹æ±‚
   */
  async generate(
    prompt: string,
    model: string = this.config.defaultModel,
    options: Partial<OllamaRequest['options']> = {}
  ): Promise<OllamaResponse> {
    const request: OllamaRequest = {
      model,
      messages: [
        {
          role: 'user',
          content: prompt
        }
      ],
      stream: false,
      options: {
        temperature: 0.7,
        ...options
      }
    }
    
    return this.chat(request)
  }

  /**
   * å–å¾—å¯ç”¨æ¨¡å‹åˆ—è¡¨
   */
  async listModels(): Promise<string[]> {
    try {
      const response = await fetch(`${this.config.baseUrl}/api/tags`, {
        signal: this.controller.signal
      })
      
      if (!response.ok) {
        throw new Error(`HTTP ${response.status}: ${response.statusText}`)
      }
      
      const data = await response.json()
      return data.models.map((m: any) => m.name)
      
    } catch (error) {
      console.error('ç²å–æ¨¡å‹åˆ—è¡¨å¤±æ•—:', error)
      
      // è¿”å›é è¨­æ¨¡å‹åˆ—è¡¨
      return [
        'gemma3:4b',
        'gpt-oss:20b',
        'llama3.1',
        'qwen2.5',
        'deepseek-coder'
      ]
    }
  }

  /**
   * æª¢æŸ¥ Ollama æ˜¯å¦å¯ç”¨
   */
  async healthCheck(): Promise<boolean> {
    try {
      const response = await fetch(`${this.config.baseUrl}/api/tags`, {
        signal: AbortSignal.timeout(5000)
      })
      
      return response.ok
      
    } catch (error) {
      console.warn('Ollama å¥åº·æª¢æŸ¥å¤±æ•—:', error instanceof Error ? error.message : String(error))
      return false
    }
  }

  /**
   * å–æ¶ˆæ‰€æœ‰é€²è¡Œä¸­çš„è«‹æ±‚
   */
  cancel(): void {
    this.controller.abort()
  }

  private async sendRequest(
    request: OllamaRequest,
    options: { signal?: AbortSignal }
  ): Promise<OllamaResponse> {
    const { baseUrl } = this.config
    const signal = options.signal || this.controller.signal
    
    const response = await fetch(`${baseUrl}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify(request),
      signal
    })
    
    if (!response.ok) {
      let errorMessage: string
      
      try {
        const errorData = await response.json()
        errorMessage = errorData.message || errorData.error || response.statusText
      } catch {
        errorMessage = `HTTP ${response.status}: ${response.statusText}`
      }
      
      throw new OllamaError(errorMessage, response.status)
    }
    
    return response.json()
  }

  private createTimeoutPromise(): Promise<OllamaError> {
    return new Promise((_, reject) => {
      setTimeout(() => {
        reject(new OllamaError(`è«‹æ±‚é€¾æ™‚ (${this.config.timeout}ms)`, undefined, true))
      }, this.config.timeout)
    })
  }

  private async delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms))
  }

  private shouldRetry(error: any): boolean {
    // å¦‚æœæ˜¯ OllamaErrorï¼Œæª¢æŸ¥æ˜¯å¦å¯é‡è©¦
    if (error instanceof OllamaError) {
      return error.isRetriable
    }
    
    // ç¶²è·¯éŒ¯èª¤æˆ–é€¾æ™‚å¯é‡è©¦
    if (
      error instanceof TypeError || // ç¶²è·¯éŒ¯èª¤
      error.name === 'AbortError' || // é€¾æ™‚
      error.name === 'TimeoutError' || // é€¾æ™‚
      error.message.includes('timeout') ||
      error.message.includes('network')
    ) {
      return true
    }
    
    // HTTP 5xx éŒ¯èª¤å¯é‡è©¦
    if (error.statusCode && error.statusCode >= 500) {
      return true
    }
    
    return false
  }

  private normalizeError(error: any): OllamaError {
    if (error instanceof OllamaError) {
      return error
    }
    
    if (error instanceof Error) {
      const statusCode = (error as any).statusCode || (error as any).code
      const message = error.message || 'æœªçŸ¥çš„ Ollama éŒ¯èª¤'
      
      // åˆ¤æ–·æ˜¯å¦å¯é‡è©¦
      const isRetriable = this.shouldRetry(error)
      
      return new OllamaError(message, statusCode, isRetriable)
    }
    
    return new OllamaError('æœªçŸ¥çš„ Ollama éŒ¯èª¤', undefined, false)
  }

  /**
   * æ¨¡æ“¬ç”Ÿæˆï¼ˆé–‹ç™¼ç’°å¢ƒä½¿ç”¨ï¼‰
   */
  static async simulateGenerate(
    prompt: string,
    model: string = 'gemma3:4b'
  ): Promise<OllamaResponse> {
    console.log('ğŸ§ª æ¨¡æ“¬ Ollama ç”Ÿæˆ:', prompt.substring(0, 50) + '...')
    
    // æ¨¡æ“¬å»¶é²
    const delay = Math.random() * 2000 + 1000
    await new Promise(resolve => setTimeout(resolve, delay))
    
    // æ¨¡æ“¬å›æ‡‰
    return {
      model,
      created_at: new Date().toISOString(),
      message: {
        role: 'assistant',
        content: `é€™æ˜¯ä¸€å€‹æ¨¡æ“¬å›æ‡‰ï¼Œå¯¦éš›å°‡é€£çµåˆ° Ollama localã€‚
åŸå§‹ prompt: "${prompt.substring(0, 100)}..."
æ¨¡å‹: ${model}
æ¨¡æ“¬å»¶é²: ${Math.round(delay)}ms`
      },
      done: true,
      total_duration: delay,
      prompt_eval_count: prompt.length,
      eval_count: Math.floor(prompt.length / 4)
    }
  }
}

// Default instance
export const defaultOllamaClient = new OllamaClient({
  baseUrl: 'http://localhost:11434',
  defaultModel: 'gemma3:12b-cloud',
  timeout: 90000,
  maxRetries: 3,
  retryDelay: 1000
})